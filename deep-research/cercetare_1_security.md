---
conversie_roland: true
data: 2025-08-14
caractere: 73097
cuvinte: 10769
tokeni_estimati: 18275
optimizat_pentru: AI (Claude/GPT)
---

Implementarea Foundation & Security + ADN #1 (Context Specializat)
Pentru sistemul de traduceri AI descris, vom implementa complet și la standard enterprise toate funcționalitățile critice de securitate, compliance GDPR și context specializat, folosind exclusiv soluții open-source self-hosted. Abordarea țintește rezolvarea vulnerabilităților identificate (lipsa verificării webhook, rate limiting, ștergere date, etc.)[1] și adaugă mecanisme avansate de adaptare a traducerii la contextul documentului (ADN #1) pentru acuratețe sporită și monetizare superioară[2][3]. Toate componentele vor fi integrate în stack-ul existent (Flask backend, SQLite, local files) fără costuri suplimentare sau servicii cloud, conform cerințelor.
🛡️ Componenta 1: Security Fixes (Soluții Open-Source)
Componenta de securitate va consolida aplicația împotriva fraudelor și abuzurilor, acoperind trei puncte critice: verificarea semnăturii webhooks Stripe, limitarea ratei de acces și gestionarea în siguranță a upload-urilor. Aceste măsuri au fost prioritizate drept esențiale[4] pentru a preveni plăți false, atacuri DoS și breșe de fișiere. Se folosesc doar biblioteci gratuite (ex. modul hmac, Redis open-source, Flask standard) și stocare locală, respectând cerințele de zero cost.
1.1 Verificare Webhook HMAC (Stripe)
•	Validare semnătură Stripe: Implementăm un middleware Flask (sau decorator pe ruta de webhook) care verifică semnătura HMAC SHA256 a fiecărui webhook Stripe primit, folosind secretul endpoint-ului Stripe. Stripe include un header Stripe-Signature ce conține timestamp și semnături. Middleware-ul va extrage timestamp-ul și semnătura din header și va calcula HMAC-ul folosind payload-ul brut al cererii (exact cum a fost primit, fără prelucrări) și secretul cunoscut. Se folosește hashlib.sha256 și hmac (din biblioteca standard Python) pentru a genera semnătura așteptată, care apoi este comparată în mod securizat (hmac.compare_digest) cu semnătura trimisă de Stripe[5]. În acest mod ne asigurăm că evenimentul provine într-adevăr de la Stripe și nu a fost modificat în tranzit[6].
•	Toleranță timp 300s: Vom verifica și timestamp-ul semnăturii pentru a preveni replay attacks. Dacă diferența dintre timestamp (trimis de Stripe) și ora serverului depășește 300 de secunde (5 minute), webhook-ul este respins ca fiind expirat, chiar dacă semnătura HMAC corespunde[5]. Această toleranță de 5 minute este conform practicilor Stripe (default al librăriilor oficiale) și împiedică un atacator să reutilizeze o cerere veche[7].
•	Integrare cu Stripe SDK (optional): Pentru robustețe, putem folosi și metoda oferită de librăria oficială Stripe (care este gratuită) – de exemplu stripe.Webhook.construct_event(payload, sig_header, secret, tolerance=300) – care aruncă o excepție dacă semnătura nu validează. În orice caz, implementarea manuală de mai sus asigură independența de librării externe și respectă exact mecanismul HMAC Stripe.
•	Error handling și logging: Dacă verificarea eșuează (semnătură invalidă sau timp expirat), serverul va returna imediat un HTTP 400 și nu va procesa evenimentul. Vom loga incidentul într-un fișier local de jurnal (ex: logs/webhook_errors.log) cu detalii (timestamp, IP, event id, motiv) pentru audit și debugging. Logging-ul local garantează păstrarea unei urme a tentativelor eșuate, îmbunătățind capacitatea de auditare a securității.
•	Idempotency tracking: Vom introduce un mecanism de idempotency pentru webhook-uri, pentru a evita procesarea dublă a aceluiași eveniment Stripe (de ex., situația în care Stripe retrimite event-ul sau îl primim de două ori din eroare de rețea). Concret, în SQLite vom avea o tabelă processed_webhooks(event_id TEXT PRIMARY KEY, processed_at DATETIME) unde memorăm ID-urile evenimentelor Stripe deja procesate cu succes[8]. La recepția unui webhook valid, se verifică în baza de date dacă event_id există deja; dacă da, se ignoră (returnând 200 OK imediat pentru a opri retry-ul Stripe), dacă nu, se procesează și se inserează ID-ul în tabel. Acest tracking local ne protejează de double-charging sau acțiuni repetate accidental[9].
•	Rate limiting pentru webhook (dedicat): Deși webhook-urile vin de la Stripe, vom adăuga și un rate limit specific pe endpoint-ul de webhook ca strat suplimentar. De exemplu, acceptăm max. 10 cereri/minut pe ruta Stripe Webhook, ceea ce este mult peste rata normală Stripe, dar previne flood-ul deliberat (în caz că un atacator descoperă URL-ul webhook). Limitarea se va implementa tot cu Redis (vezi secțiunea 1.2) însă pe un key separat global (nu pe IP, deoarece Stripe folosește IP-uri variate) sau folosind un token global. În plus, se poate valida și IP-ul sursă al cererii pentru a accepta doar range-urile cunoscute Stripe (opțional, pentru extra securitate).
În concluzie, după acest pas, webhook-urile de plată sunt securizate: doar evenimente Stripe reale, recente și neprocesate anterior vor fi luate în calcul[10].
1.2 Rate Limiting cu Redis (Local)
•	Instanță Redis locală: Vom folosi Redis instalat local (rulează ca serviciu pe server sau container Docker) pentru a implementa limitarea de rată. Redis este open-source și foarte eficient pentru contoare cu expirare, astfel încât nu introducem dependențe cloud. Dacă se folosește Docker Compose, vom adăuga un serviciu Redis care să fie accesibil doar intern (localhost) pentru aplicația Flask.
•	Limitare globală API: Vom aplica o regulă de rate limiting de 5 cereri pe minut per adresă IP pentru endpoint-urile sensibile ale aplicației (ex: endpoint-ul de traducere efectivă sau de preview) – exact pragul recomandat anterior pentru prevenirea abuzului[8][11]. Implementarea se face astfel: la fiecare request, se formează o cheie Redis de forma rl:<IP>:<endpoint> și se execută operația atomica INCR pe acea cheie, cu setare de expirare la 60 secunde dacă cheia nu există. Dacă valoarea returnată de INCR depășește 5, atunci limita a fost atinsă și vom bloca cererea (status 429 Too Many Requests). Altfel, lăsăm execuția normală. Deoarece Redis INCR cu expirare este atomic, această soluție va funcționa corect chiar și sub concurență ridicată. Limitele concrete (5/minut) pot fi ajustate ușor dintr-un fișier de configurare YAML.
•	Limită specială pentru preview: Conform cerințelor, vom adăuga o limitare mai strictă pentru cererile de preview gratuit – de exemplu maximum 2 preview-uri pe zi per IP. Aceasta previne abuzul de previzualizări gratuite (evitând ca un utilizator neautentificat să obțină traduceri bucățite gratuit). Tehnic, implementăm similar: cheie Redis preview:<IP> cu expirare 24h și limitare la 2. Dacă un IP solicită a 3-a previzualizare în aceeași zi, i se refuză cererea cu un mesaj de tip "Limită de previzualizări atinsă, reveniți mâine". Această regulă încurajează utilizatorii să treacă la variante plătite dacă depășesc numărul de preview gratuit.
•	Whitelist clienți premium: Vom permite definirea unei liste albe de clienți (identificați după IP sau, mai bine, după un token API/client ID dacă există autentificare) care să fie scutiți de limitările de mai sus. Acești clienți premium (de ex. corporate clients sau administratorul) pot fi listați într-un fișier local de configurare (ex: config/whitelist.yaml cu IP-uri sau ID-uri exceptate). La verificarea rate-limit, dacă IP-ul se află în whitelist, regula este omisă. Astfel, clienții plătitori pot beneficia de throughput mai mare conform abonamentelor lor.
•	Banare temporară IP: Vom extinde sistemul de rate-limit cu un mecanism de ban temporar pentru IP-urile care continuă să trimită cereri excesive. De exemplu, dacă un IP a fost blocat de 3 ori într-un interval scurt (sau depășește de N ori limita), putem considera comportament malițios și îl banăm pentru o perioadă (ex: 1 oră). Implementare: menținem în SQLite o tabelă banned_ips(ip, ban_until). Când un IP atinge pragul de ban, inserăm/actualizăm intrarea cu timestamp-ul de expirare a ban-ului (ex: current_time + 1 oră). La fiecare request, înainte de verificarea Redis, consultăm SQLite pentru a vedea dacă IP-ul este listat ca banat și dacă perioada nu a expirat. Dacă da, returnăm direct 429/403 și nu incrementăm contorul. Acest ban persistă și dacă serverul repornește (datorită stocării pe disc). Unscript de unban automat poate rula periodic (sau la fiecare request putem șterge intrările expirate). Astfel, un atacator va fi temporar blocat după abuz repetat, protejând infrastructura.
•	Implementare Flask: Putem integra rate-limiter-ul folosind fie un middleware WSGI care interceptează toate request-urile (și decide pe baza PATH ce reguli se aplică), fie folosind un decorator @limit pe anumite rute. Există și extensii ca flask-limiter, dar vom implementa manual pentru control deplin și dependențe minime. Logica de Redis și ban va fi încapsulată eventual într-un utilitar rate_limit.py apelat din routes.
•	Logging și configurare: Toate evenimentele de limită depășită sau ban vor fi logate (în JSON log, vezi secțiunea 2.3) cu detalii (IP, endpoint, acțiune luată) pentru analiză ulterioară. Limitele (numere, ferestre de timp), durata ban-ului și whitelist-ul sunt configurabile via YAML, astfel încât pot fi ușor ajustate fără modificare de cod.
Prin această componentă, sistemul va preveni suprasolicitarea și atacurile de tip brute-force/DDoS la nivel de API, conform best-practices. Limita de 5 req/min/IP propusă previne abuzul fără a impacta utilizatorii legitimi obișnuiți[11].
1.3 Upload Local cu Validare și Securizare
•	Încărcare direct pe server (fără cloud): În locul folosirii unor URL-uri semnate S3 (cum era recomandat pentru securitate în variantă enterprise)[10], vom permite upload-ul fișierelor direct pe serverul local, însă cu măsuri stricte de validare server-side. Fișierele încărcate vor fi salvate într-un director local securizat (ex. uploads/[12]) pe același server, eliminând dependențele de servicii externe de stocare. Directorul uploads/ va fi protejat astfel încât fișierele să nu fie accesibile direct prin URL public (neconfigurat ca static serving), ci doar prin procesele interne de traducere/livrare.
•	Validare tip MIME și extensie: La primirea unui fișier, serverul va verifica că tipul de fișier este permis și corespunde conținutului. Se definește o listă de extensii acceptate (e.g. .pdf, .docx, .xlsx, .txt, .csv etc. – sistemul suportă mai multe formate[13]). Vom folosi werkzeug.utils.secure_filename pentru a normaliza numele fișierului (înlăturând caractere periculoase) și vom verifica extensia. În plus, pentru siguranță, determinăm și tipul MIME real al fișierului – fie folosind Python mimetypes/python-magic (dacă e instalabil gratuit) – pentru a preveni situații de tip “polyglot” (ex: fișier .pdf fals care este de fapt script). Dacă tipul real nu corespunde cu extensia așteptată sau nu este în whitelist, upload-ul este respins cu eroare. De asemenea, se impune și o limită de mărime (ex: max 50MB per fișier, configurabil) – fișierele mai mari vor fi refuzate cu un mesaj de eroare, prevenind atât abuzul de stocare cât și eventuale probleme de performanță.
•	Stocare securizată: Fișierul validat este salvat în uploads/ sub un nume unic. Ca bună practică, putem genera un UUID sau hash din conținut ca nume de fișier, în loc să folosim numele original, evitând coliziuni și ascunzând informații despre fișier. De exemplu, dacă userul încarcă Contract.docx, îl salvăm ca uploads/abcd1234.docx și reținem maparea în baza de date (sau în sesiune). Astfel, chiar dacă cineva ar ghici calea, nu poate obține fișierul fără un ID greu de ghicit. În plus, vom seta permisiunile fișierelor pe disc astfel încât doar userul sistem (sau procesul Flask) să poată citi/șterge, și nu sunt executabile.
•	Integrare cu backend-ul existent: Procesul de upload va fi gestionat probabil de o rută Flask /upload definită în backend/app.py sau un blueprint dedicat. După salvare, se va apela logica de procesare document (document_processor.py) pentru a extrage textul și calcula numărul de cuvinte/pagini[14]. Fluxul continuă apoi cu generarea preview-ului și afișarea costului. Adaptăm minimal codul existent: înlocuim eventualul upload anterior (dacă era direct în memorie sau alt sistem) cu acesta local, asigurând compatibilitatea cu restul pipeline-ului.
•	Cleanup automat al fișierelor: După ce fișierul a fost procesat și tradus, vom decide politica de retenție conform GDPR (vezi 2.1). Pe termen scurt, putem șterge imediat fișierele temporare care nu mai sunt necesare (ex: fișiere intermediare de extragere text, dacă există). Pe termen mediu, fișierul sursă original rămâne stocat până la 30 de zile (conform retenției) pentru posibile download-uri ulterioare sau verificări, urmând a fi șters automat. De asemenea, dacă un utilizator anulează comanda sau nu finalizează plata, fișierul încărcat poate fi eliminat mai devreme (ex: după 24h) pentru a elibera spațiu, întrucât nu mai e nevoie de el.
•	Fără dependențe externe riscante: Gestionarea fișierelor va fi realizată doar cu cod Python nativ și biblioteci standard (os, shutil etc.). Nu vom folosi servicii third-party de scanare sau stocare cloud. Opțional, pentru securitate sporită, se poate integra un antivirus open-source (ClamAV) pe server pentru a scana fișierele încărcate de viruși macro sau scripturi, însă acest pas este opțional și poate fi menționat în checklist-ul de securitate.
•	Logging upload & errors: Fiecare upload va fi logat (nume original, noul ID, user/email, timp) în log-ul audit, iar erorile de validare (tip invalid, oversize) vor fi și ele înregistrate pentru a monitoriza tentativele eșuate sau potențiale atacuri (ex: cineva încercând să urce .exe).
Prin aceste măsuri, upload-ul local devine securizat: acceptăm doar tipuri de fișiere cunoscute și sigure, prevenim injectarea de conținut malitios și stocăm fișierele în mod izolat. Chiar dacă inițial se recomanda upload direct în cloud cu URL presigned pentru siguranță[10], soluția noastră locală oferă un nivel similar de securitate prin validare strictă și izolare, fără costuri.
🛡️ Componenta 2: GDPR Compliance (Basic, Self-Hosted)
Pentru alinierea la GDPR, implementăm mecanisme de ștergere automată a datelor, portal de acces/ștergere a datelor pentru clienți (DSAR) și logging audit – toate local, fără servicii externe, conform cerințelor. Aceste funcții acoperă lipsurile de conformitate identificate[15] și asigură că datele personale sunt gestionate transparent și șterse la termen[16]. Vom folosi scripturi Python (cron jobs locale), SQLite și fișiere JSON/CSV pentru a realiza aceste cerințe gratuit.
2.1 Retenție Automată a Datelor (Auto-Delete)
•	Ștergere fișiere după 30 zile: Vom implementa o politică de retenție conform recomandărilor – fișierele încărcate de clienți și rezultatele traducerii vor fi păstrate maxim 30 de zile pe server, după care se șterg automat[17]. Acest interval acoperă nevoile uzuale (clientul poate descărca traducerea sau cere rectificări într-o lună) și limitează expunerea datelor pe termen lung. Implementare: un cron job zilnic (sau folosind un scheduler Python) va parcurge directorul uploads/ și alte foldere relevante (ex. processed/ pentru traduceri livrate) și va șterge fișierele mai vechi de 30 de zile (comparând timestamp-ul fișierului cu data curentă). Vom putea folosi o bibliotecă precum schedule sau APScheduler (free) în modul daemon al aplicației, sau alternativ un script separat cleanup.py pus în crontab al serverului, care rulează zilnic noaptea. Orice fișier șters va fi notat în log-ul de audit cu data și identificator (pentru evidență).
•	Ștergere/anonomizare metadata după 365 zile: Conform GDPR, datele personale din sistem (metadate precum informații despre comenzi, email-uri de contact, istoricul traducerilor) vor fi păstrate cel mult 1 an după care eliminate sau anonimizate[17]. Implementare: în SQLite, vom marca intrările mai vechi de 365 zile pentru ștergere. De exemplu, tabele precum orders, payments, logs vor fi curățate de toate rândurile mai vechi de un an. Pentru a nu pierde eventuale statistici agregate, se poate în loc de ștergere directă să se anonimizeze unele câmpuri (ex: înlocuire email cu hash sau “DELETED”) astfel încât datele să nu mai fie personale, dar să poată fi folosite la raportări generale. Totuși, conform cerinței ne vom concentra pe ștergere completă. Un script similar (cron zilnic sau lunar) se va ocupa de acest proces pe DB.
•	Soft delete și backup local: Vom introduce conceptul de „soft delete” pentru un interval scurt, pentru a permite eventuale recuperări dacă ștergerea s-a făcut din greșeală. Concret, înainte de ștergerea definitivă, datele vor fi backup-ate local pe termen scurt. De exemplu, fișierele ce urmează a fi șterse pot fi mutate într-un director backup_deleted/ (sau arhivate .zip) cu un timestamp, și păstrate acolo încă ~7 zile înainte de ștergerea finală. Similar, înainte de a șterge rânduri din SQLite, le putem copia într-o tabelă de backup (sau exporta într-un fișier CSV/JSON) stocată local pentru o perioadă scurtă. Acest mecanism asigură posibilitatea de restaurare a datelor șterse recent dacă, de exemplu, un client solicită recuperarea din greșeală. După perioada de grație (ex. 7 zile), backup-urile respective sunt și ele șterse automat. Toate aceste acțiuni vor fi consemnate în audit log.
•	Audit trail ștergeri: Fiecare execuție de retenție va fi logată: vom crea un fișier de audit (sau adăuga într-un JSON log comun) evenimente de tip "DataRetentionDeletion" conținând ce s-a șters (ex: "user X – șters fișier Y uploadat la data Z" sau "șters înregistrări comenzi din <2024") și timestamp-ul. Acest audit trail completează cerința de a păstra evidența acțiunilor asupra datelor personale[18].
•	Configurabil și extensibil: Valorile de retenție (30 zile, 365 zile) vor fi puse în fișierul de configurare YAML, pentru a le putea modifica ușor în viitor (de exemplu, dacă se schimbă politica de companie sau cerințele legale). Sistemul de cleanup poate fi extins să șteargă și alte tipuri de date temporare (ex: token-uri expirate, log-uri vechi mai mult de X timp, etc., deși log-urile pot fi păstrate mai mult în scop de securitate).
Implementând retenția automată, ne asigurăm că nu stocăm date personale mai mult decât e necesar, îndeplinind cerințele GDPR de minimizare a perioadei de stocare. Acest lucru evită și încărcarea inutilă a serverului cu fișiere vechi.
2.2 Portal Client DSAR (Export/Ștergere Date)
•	Interfață self-service: Vom crea un portal web securizat unde clienții își pot exercita drepturile GDPR: să descarce toate datele lor stocate sau să solicite ștergerea completă a datelor. Conform specificației, vom implementa acest portal local, ca parte a site-ului (fără servicii externe). În pagina principală sau într-o secțiune dedicată (ex: "GDPR/Data Request"), utilizatorul poate alege: Exportă datele mele sau Șterge datele mele.
•	Identificare prin email: Pentru a identifica datele utilizatorului, vom folosi adresa de email (pe care clientul a furnizat-o la plasarea comenzii/traducerii). Formularul DSAR va cere utilizatorului să introducă emailul folosit în sistem. Deoarece nu avem conturi cu autentificare, vom verifica identitatea printr-un cod de confirmare trimis pe email înainte de a livra datele sau a le șterge, prevenind ca cineva să încerce accesul pe baza emailului altcuiva. Acesta este un proces simplu de verificare prin email, conform cerinței[17].
•	Flux export date (Download ZIP): Dacă utilizatorul alege exportul datelor:
•	După introducerea email-ului, serverul generează un token unic (un UUID) asociat cererii și îl salvează în SQLite (tabel dsar_requests cu email, token, tip cerere, expirație).
•	Se trimite un email către adresa respectivă conținând un link securizat de download, de forma https://siteul-meu/DSAR/export?token=<UUID>. Acest email va fi expediat folosind un SMTP gratuit – putem folosi contul propriu Gmail prin smtplib (cu autentificare pe baza unui app password) sau un server SMTP local. Implementarea trimiterii emailului se face în modul asincron (se poate folosi thread separat sau Celery task, dacă disponibil, pentru a nu bloca răspunsul).
•	Când userul primește emailul și dă click pe link, serverul validează token-ul: caută token-ul în DB, verifică dacă este valid și neexpirat (setăm o valabilitate de ~24 ore).
•	Dacă valid, sistemul pregătește toate datele utilizatorului într-un pachet ZIP. Aceasta include: toate fișierele încărcate de utilizator (dacă mai există în uploads/ sub 30 zile), traducerile rezultate (dacă există fișiere de ieșire stocate), și un fișier .json sau .csv cu metadatele – de ex. conținutul tuturor înregistrărilor din SQLite legate de email-ul lui (comenzi, plăți, log-uri relevante). Vom realiza exportul prin interogări SQLite pe tabele (SELECT * FROM orders/payments where email=X) și scrierea rezultatelor într-un format tabular sau JSON ușor de înțeles.
•	Folosim Python zipfile pentru a adăuga toate aceste date în arhivă. De exemplu, data_export_<email>_<date>.zip va conține: orders.csv, translations_list.csv, personal_data.json și eventual subfoldere files/ cu documentele originale și traducerile (dacă disponibile).
•	Apoi livrăm acest ZIP fie direct ca răspuns la download (cu send_file Flask, application/zip), fie oferim un link de download în pagină. După download, din motive de securitate, putem invalida imediat token-ul (și eventual șterge arhiva temporară dacă am salvat-o pe disc). Astfel, datele ajung la user în mod securizat.
•	Flux ștergere date: Dacă utilizatorul solicită ștergerea:
•	Procedura de autentificare este similară: introduce email, primește un link de confirmare ...?token=XYZ pentru ștergere.
•	Când accesează link-ul, token-ul e verificat. Pentru ștergere, vom cere o confirmare suplimentară (ex: o pagină "Ești sigur că vrei să ștergi permanent toate datele?" cu buton "Confirmă ștergerea"), ca măsură de siguranță împotriva click-urilor accidentale din email.
•	La confirmare, serverul va identifica toate datele asociate acelui email în sistem:
o	Fișiere încărcate în uploads/ (și eventual traduceri în processed/): le va șterge imediat (folosind aceleași funcții ca la retenția automată, dar forțat acum). Înainte de ștergere definitivă, putem aplica și aici soft delete – de exemplu mutăm fișierele într-un backup special (criptat eventual) pentru o perioadă scurtă, în caz de dispută, dar oficial față de client le considerăm șterse.
o	Înregistrări în baza de date (comenzi, plăți, memorie traduceri, preferințe, log-uri ce conțin emailul): le va șterge sau anonimiza. Vom rula comenzi DELETE pe tabelele relevante cu condiția email = client. Cheile străine și integritatea referențială trebuie avute în vedere; dacă există, se vor șterge în ordinea corectă sau cu CASCADE. Alternativ, dacă dorim păstrarea unor statistici anonime (ex: număr de cuvinte traduse), putem în loc să ștergem intrările din orders să le anonimizăm (email devine "deleted_user_123"), însă conform cerinței vom realiza ștergere completă.
•	După ștergere, vom trimite un email automat de confirmare către client: un mesaj că "Toate datele asociate adresei dvs. au fost șterse definitiv la data... Dacă aveți alte solicitări bla bla".
•	În log-ul de audit, înregistrăm acțiunea de ștergere: cine (email) a fost șters, la ce oră, și ce categorii de date au fost eliminate.
•	Securitate și restricții: Link-urile de confirmare au un token unic dificil de ghicit și sunt valabile limitat în timp. Odată folosite sau expirate, ele sunt șterse din DB. De asemenea, vom implementa o limitare: dacă cineva introduce un email care nu există în sistem, vom afișa același mesaj generic ("Dacă adresa există, veți primi un email..."), ca să nu divulgăm informația că un anumit email e sau nu în baza noastră (prin asta evităm un potențial leak de existență a datelor). Pentru emailul de trimitere vom folosi un cont propriu (SMTP Gmail configurat în config.yaml cu user/parolă sau un SMTP local dacă disponibil) – această soluție este gratuită. SMTP-ul trebuie configurat cu TLS (smtplib se ocupă de asta) pentru confidențialitatea mesajelor.
•	Implementare modulară: Vom crea probabil un modul dsar_service.py care conține funcții: send_export_email(email), send_delete_email(email), perform_export(token) și perform_delete(token). Rutele Flask /request-export și /request-delete vor primi formularul inițial, iar rutele /export?token= și /delete?token= vor realiza acțiunile finale. Acest design separă clar logica și permite testare ușoară.
Cu acest portal DSAR, respectăm cerințele GDPR de acces și ștergere la cerere a datelor[16], oferind utilizatorilor control și transparență totală. Totul este self-hosted și custom, fără costuri suplimentare, dar oferind o experiență profesională (email automat, arhivă completă de date).
2.3 Audit Logging în Fișiere (Traceabilitate)
•	Jurnalizare în format JSON: Vom introduce un sistem de logare detaliată a evenimentelor aplicației, pentru a avea un audit trail complet al acțiunilor (cum solicită GDPR și bunele practici de securitate)[15]. Log-urile vor fi stocate în fișiere pe disc (directorul logs/[19]) și vor fi în format JSON pentru a facilita procesarea ulterioară. Fiecare eveniment va fi reprezentat ca un obiect JSON pe o linie (JSON Lines), conținând câmpuri precum timestamp, tip eveniment, detalii relevante (user, IP, id comanda etc.). Acest format permite cu ușurință să fie filtrat sau transformat ulterior (ex: importat într-un pandas DataFrame pentru analiză).
•	Evenimente logate: Vom loga automat următoarele categorii de acțiuni:
•	Securitate: orice autentificare webhook eșuată, orice block de rate-limit sau ban, încercări de upload respinse, accesări neautorizate (de ex. cineva încearcă un URL inexistent).
•	Tranzacții și plăți: recepție webhook Stripe valid (cu ID tranzacție), plăți confirmate, erori de plată.
•	Acțiuni utilizator: upload fișier (cu meta ex: nume, tip), generare preview, realizare traducere finală, descărcare fișier.
•	DSAR: cerere export inițiată, export livrat, cerere ștergere inițiată, ștergere efectuată (incluzând ce date s-au șters).
•	Sistem: ștergeri automate (cu număr de fișiere șterse), backup realizat, pornire/oprire servicii, erori neprevăzute (traceback-uri).
Practic, orice acțiune care modifică date sau este relevantă pentru audit va genera o intrare log. Acest lucru îndeplinește cerința de audit logging complet pentru fiecare acțiune[18].
•	Rotirea și comprimarea log-urilor: Pentru a evita ca fișierele de log să devină foarte mari și pentru a organiza istoricul, vom implementa un mecanism de rotating logs. Vom crea fișiere separate fie pe categorii, fie pe interval de timp:
•	De exemplu, un fișier logs/audit_2025-08-15.json pentru activitatea zilei respective, care a doua zi se închide și se pornește unul nou. Sau putem folosi un singur log audit și să-l rotim când depășește o dimensiune (ex: 10 MB) sau zilnic la miezul nopții. Biblioteca logging din Python are un RotatingFileHandler și TimedRotatingFileHandler care pot fi folosite pentru ușurință, dar putem și manual.
•	După rotire, fișierele de log vechi vor fi comprimat automat (e.g. se poate configura ca fișierul închis să fie arhivat .gz sau .zip). Astfel, audit_2025-08-14.json devine audit_2025-08-14.json.gz pentru a economisi spațiu.
•	Vom păstra log-urile comprimate local atâta timp cât e necesar (poate 1-2 ani, sau conform politicii interne) deoarece log-urile nu conțin neapărat date personale (dacă conțin emailuri, totuși intră sub GDPR și atunci reținem max 1 an și pentru ele). Se poate aplica și pe ele aceeași curățare după 365 zile, eventual.
•	Export și analiză log-uri: Vom furniza scripturi utilitare pentru a lucra cu aceste log-uri:
•	Un script Python (ex: tools/logs_to_csv.py) care citește fișierele JSON și produce un CSV agregat (sau rapoarte sumare). Acesta ajută la audit extern sau la analiză managerială.
•	Un alt script tools/search_logs.py care permite filtrarea logurilor după criterii – ex: după email-ul userului, după tip eveniment, dată etc. Acesta poate parcurge fișierele (inclusiv pe cele .gz) și afișa intrările relevante. Acest tool ușurează răspunsul la întrebări de tip: "să găsim toate acțiunile legate de user X în ultimele 6 luni" sau "toate upload-urile respinse ca tip invalid".
•	De asemenea, includem un script de backup a log-urilor: lunar, toate logurile lunii precedente pot fi arhivate într-un logs_archive_2025-08.tar.gz și mutate într-un director separat sau pe un storage extern (dacă se configurează unul local). Acest proces poate fi integrat în jobul de retenție sau rulat manual.
•	Implementare în cod: Vom configura logger-ul global al aplicației Flask să folosească un custom formatter pentru JSON (sau vom loga manual evenimentele critice). E relativ ușor să facem un dict și să-l serializăm cu json.dumps() în fișier. Putem crea un helper audit_log(event_type, details_dict) care să atașeze timestamp automat și să scrie în fișierul curent. Pentru logarea request-urilor, Flask oferă after-request handlers – putem intercepta răspunsurile 4xx/5xx să le logăm. Însă majoritatea evenimentelor le vom loga direct în locurile din cod unde se întâmplă (ex: după ce ștergem datele cuiva, apelăm audit_log("DSAR_DELETE", {"email": user_email, "records_deleted": X, "files_deleted": Y})).
Prin acest sistem de logging, obținem transparență totală și posibilitatea de audit a sistemului. În caz de incidente (de ex. o plângere GDPR), putem demonstra exact cine și când a accesat sau șters date[20]. De asemenea, log-urile ajută la debugging și la monitorizarea sănătății aplicației în exploatare.
🧠 Componenta 3: ADN #1 – Context Specializat (AI Adaptation)
Componenta ADN #1 introduce inteligența contextuală în traduceri – sistemul va detecta automat domeniul textului și va adapta traducerea (prin prompt-uri, glosare locale și post-procesare) pentru a menține acuratețea terminologică. De asemenea, vom implementa funcționalități conexe: pricing diferențiat în funcție de context (domeniile specializate au cost ușor mai ridicat) și memorie client (învățarea preferințelor și reutilizarea traducerilor anterioare). Scopul este creșterea calității și oferirea de opțiuni premium (ex. pachete pe industrii) – idee aliniată cu Industry Packs și Smart Pricing din planul enterprise[3][21]. Toate soluțiile folosite sunt locale (algorithmic, configurări) fără API-uri plătite suplimentare, folosind motoarele existente (DeepL/Google) într-un mod optimizat.
3.1 Detectare Automată a Contextului (Algoritm Bag-of-Words)
•	Domenii suportate: Vom defini o listă de ~8 domenii posibile pentru contextul documentului, conform celor discutate anterior[22][23]. De exemplu: General, Tehnic/Ingineresc, IT/Informatica, Juridic, Medical, Business/Economic, Sportiv, Academic etc. Aceste domenii acoperă categoriile majore identificate ca relevante pentru traduceri specializate. Lista exactă și denumirile pot fi ajustate; în knowledge base s-au propus liste extinse (Matematică & Statistică, Informatică & IT, Legal, Medical, Marketing, etc.) pe care ne vom baza[23].
•	Dicționare de cuvinte-cheie: Pentru fiecare domeniu vom pregăti un dicționar local de termeni reprezentativi. Acestea pot fi fișiere JSON separate (ex: keywords_medical.json, keywords_legal.json, ...) sau un singur fișier YAML care conține sub-chei pe domenii cu lista de cuvinte asociate. Termenii vor include jargon specific domeniului respectiv. De pildă, pentru medical: termeni precum “clinic, pacient, terapie, simptom, diagnostic, mg, ml, etc.”; pentru juridic: “contract, lege, articol, alin., instanță, obligație, testament, etc.”; pentru IT: “algoritm, server, cod sursă, runtime, AI, rețea” etc. Putem compune aceste liste manual și le putem extinde în timp. Acest approach bag-of-words simplu ne oferă un indicator al domeniului pe baza frecvenței cuvintelor specifice.
•	Analiza primelor N cuvinte: Vom rula algoritmul de detecție pe primele ~500 de cuvinte ale textului (sau primele 1-2 pagini) – premisă: primele paragrafe sunt de obicei suficiente pentru a deduce contextul general. Limitarea la N cuvinte reduce costul computațional și evită bias-ul dat de porțiuni de text care poate nu sunt relevante (ex. anexe). Pentru extragerea textului deja există componenta de procesare document (OCR, parser PDF/DOCX)[13]; vom folosi rezultatul acela (text brut sau partial) ca input.
•	Tokenizare și curățare: Folosim Python NLTK (gratuit) pentru a tokeniza textul și eventual a elimina stopwords (cuvinte foarte comune gen “și, de, la” care nu ajută la identificare). NLTK are liste de stopwords pentru limba română și engleză – le vom folosi pentru a filtra rușii. Rulăm totul lower-case pentru a uniformiza. Putem, de asemenea, să extragem radicalul cuvintelor (stemming) pentru a prinde variații (ex: "legal"/"legale"/"legislație" – un stemmer românesc sau pur și simplu includem variații în liste).
•	Scor de frecvență: Pentru fiecare domeniu calculăm un scor bazat pe frecvența cuvintelor-cheie din lista sa ce apar în text. De exemplu, numărăm de câte ori apar cuvintele din keywords_medical în text, aceasta va fi scorul pentru medical. Putem normaliza scorul la lungimea textului (dar cum vom compara scorurile între domenii, lungimea e aceeași oricum pentru toți, deci nu e necesară normalizare complicată). Putem totuși să acordăm ponderi diferite unor termeni: de exemplu, un cuvânt foarte specific (ex: “Habeas corpus” pentru juridic, sau “appendicitis” pentru medical) ar putea conta drept +5 puncte, pe când unul mai comun (“doctor”) +1. Inițial vom simplifica luând doar frecvența brută.
•	Determinarea domeniului predominant: Comparăm scorurile tuturor domeniilor. Dacă un domeniu are un scor semnificativ mai mare decât celelalte, îl alegem ca context detectat. Trebuie stabilit și un prag de încredere. De exemplu, dacă scorul maxim reprezintă <70% din totalul cuvintelor relevante găsite sau diferența față de al doilea scor e mică, putem clasa documentul ca "General" (fără un context special clar)[24]. Pragul de 70% e empiric – putem ajusta după teste. Situații:
•	Dacă textul conține mulți termeni tehnici amestecați (ex: un raport științific poate avea atât termeni tehnici cât și juridici în preambul), algoritmul poate detecta 2 scoruri apropiate; atunci default "General" e mai sigur decât o alegere greșită.
•	Dacă niciun domeniu nu are vreun hit (text foarte general), clar va rezulta "General".
•	Dacă un domeniu domină (ex: 10 termeni medicali vs 0 din altele), luăm "Medical" cu încredere ridicată.
•	Fallback la selecția utilizatorului: Notăm că în interfață vom oferi oricum utilizatorului un dropdown de selecție a domeniului (ex: "Context Document: General / Juridic / Medical / ...")[22]. Detectarea automată va fi folosită inițial pentru a pre-selecta domeniul în dropdown sau pentru a sugera utilizatorului contextul. Clientul are opțiunea să schimbe dacă detectarea nu a nimerit (ex: îl lăsăm să selecteze manual alt domeniu). Dacă utilizatorul schimbă manual, vom folosi alegerea sa și putem actualiza și modul nostru de învățare (ex: dacă sistemul a detectat "General" dar user a selectat "Legal", putem re-evalua listele sau pur și simplu memora preferința pentru acel user – vezi Memory Client).
•	Integrare cu pipeline-ul: Odată stabilit contextul (fie auto, fie manual de la client), îl vom transmite mai departe modulului de traducere (3.2) și modulului de calcul cost (3.3). De exemplu, salvăm contextul selectat în obiectul de comandă curent (sau într-o variabilă de sesiune) pentru a fi folosit la traducere. Totodată îl stocăm în DB (tabel orders.context) pentru analytics ulterioare (să vedem ce pondere de documente sunt pe domenii specializate).
Acest algoritm bag-of-words este simplu dar eficient pentru a oferi o detecție rapidă a domeniului fără modele ML complexe. Fiind complet local și personalizabil (putem îmbunătăți listele de cuvinte în timp), asigură o adaptare a traducerii la contextul potrivit în majoritatea cazurilor. Astfel, sistemul nostru poate oferi traduceri mai precise pentru termeni tehnici și stil adecvat fiecărui domeniu (ex: ton formal juridic vs colocvial în sport) – beneficii subliniate și în planificarea inițială[25].
3.2 Adaptarea Motorului de Traducere cu Prompting (DeepL/Google)
•	Configurare context specializat: Vom crea o configurație centrală (de exemplu un dict CONTEXT_SETTINGS în cod sau un fișier YAML) care mapează fiecare domeniu la anumite parametri de traducere și acțiuni specifice. Aceasta a fost deja schițată în knowledge base ca un snippet exemplu[26]. De exemplu:
•	CONTEXT_SETTINGS['juridic'] = { "deepl_formality": "more", "glossary": "legal_ro_en.txt", "prompt": "Tradu textul în stil juridic, cu limbaj formal și termeni legali preciși." }
•	CONTEXT_SETTINGS['medical'] = { "deepl_formality": "more", "prompt": "Traducere context medical - păstrează terminologia anatomică exactă și unitățile de măsură.", "terminology_enforcement": true }
•	CONTEXT_SETTINGS['informatica'] = { "deepl_formality": "less", "code_preservation": true, "prompt": "Translate technical text preserving code and symbols unchanged." }
etc. Aceste setări vor fi folosite atât înainte de traducere (prompting, parametri API) cât și după (post-procesare regex).
•	Pre-procesare (context hints): Înainte de a trimite textul la API-ul de traducere (DeepL sau Google Translate), aplicăm transformări ușoare pentru a ghida traducătorul:
•	Formatare/Tagging special: Dacă domeniul implică părți care nu trebuie traduse, le vom marca. Exemplu: pentru domeniul IT, detectăm bucăți de cod sursă sau comenzi (poate cu regex pentru segmente în backticks, sau secvențe ce arată a cod) și le înlocuim temporar cu un token sau le încadrăm în tag-uri <code>...</code>. Dacă folosim DeepL API cu tag_handling="xml" și specificăm că tag-ul <code> e non-translatable, DeepL va lăsa conținutul neschimbat[27]. Asta asigură că int main() rămâne int main(), nu devine principal.
•	Similar, pentru matematică, am putea încadra formulele în <formula>...</formula> și marca tag-ul ca netraductibil. De exemplu, expresii cu multe cifre, simboluri speciale sau LaTeX.
•	Adăugare prompt context în text: Dacă API-ul nu suportă parametri direcți de ton/hints (Google Translate standard nu are, DeepL are formality limitat), putem adăuga manual un scurt prompt în text. Strategie: concatenăm la începutul textului original o frază de context între paranteze pătrate sau alt marker care apoi îl vom elimina. Exemplu: pentru domeniul juridic, prefațăm textul cu [Context: Document juridic, limbaj formal] sau chiar într-o variantă engleză dacă traducem spre engleză. Traducătorul neuronal poate astfel adapta stilul. Important: după primirea traducerii, vom elimina sau ajusta partea de prompt dacă a fost tradusă. Această metodă e empirică și s-ar putea ca motorul de traducere să traducă promptul ca atare; vom experimenta. În cazul DeepL, putem folosi parametri formality direct pentru limbile suportate (ex: în API, formality: "formal").
•	Glossar (când posibil): Dacă avem un mic glosar de termeni specifici domeniului (ex: Legal – "contract"->"contract", "procuror"->"prosecutor" etc.), pentru DeepL Pro se pot crea glosare custom. Însă presupunând că folosim plan free sau Google, vom implementa glosarul tot manual: înainte de traducere, putem înlocui termenii sursă care nu vrem traduși cu un placeholder (ex: "#TERM1#"), și după traducere punem înapoi termenul original sau traducerea dorită. De ex., dacă vrem să ne asigurăm că "EU" ca abreviere de Uniunea Europeană nu se traduce "SUA" din greșeală, o putem înlocui cu "#EU#". Asta echivalează cu a da hints motorului să nu traducă anumite entități.
•	Trimitere către API: Odată pre-procesat textul, îl vom transmite către DeepL (dacă cheia API este disponibilă) sau Google Translate API (dacă are credite gratuite). Vom folosi preferabil DeepL ca prim motor (calitate mai bună), și Google ca fallback[28]. Implementarea actuală are deja integrarea cu DeepL Pro și Google Translate în translation_service.py[29] – vom extinde acea logică: adăugăm parametrii contextuali. În cazul DeepL, putem seta parametru formality conform CONTEXT_SETTINGS[domeniu]['deepl_formality']. Pentru Google, nu există parametri de formalitate, dar traducând în RO putem eventual seta modelul dacă ar exista vreo opțiune (nu prea în free, decât default). Oricum, trimitem textul (cu eventualul prompt inclus).
•	Trebuie menționat că folosirea acestor API nu implică costuri dacă rămânem în limita planului gratuit: DeepL are un free tier (max 500k caractere/lună) care ne poate ajunge la început, iar Google Translate are ~500$ credit la activare. Deci în contextul "fără costuri", putem fie limita volumul, fie folosi aceste planuri gratuite. O alternativă complet open-source ar fi folosirea unui model de traducere open (ex MarianNMT sau Argos Translate offline), dar calitatea ar scădea. Ca atare, vom continua cu DeepL/Google asumând utilizarea judicioasă a planurilor gratuite.
•	Post-procesare traducere: După obținerea textului tradus de la API, vom aplica post-procesări pentru terminologie:
•	Înlocuire placeholder-e originale: Orice secvențe protejate în pre-procesare (ex: cod în <code>, formule, entități "#TERM#") le vom restaura acum în textul tradus. De obicei, DeepL respectă tag-urile și ni le returnează intacte, deci e simplu: scoatem tag-urile <code> și păstrăm conținutul. Pentru placeholders gen "#EU#", căutăm acel token în string-ul tradus și punem "UE" (sau ce decidem conform glosar).
•	Regex pentru nomenclatură: Putem rula expresii regulate pentru a corecta formatele specifice: de exemplu, în textul tradus, asigură-te că numeralele și unitățile au fost păstrate corect (ex: "12mm" nu trebuie să devină "12 mm" sau "12 milimetri" dacă nu dorim asta; pentru un document tehnic poate vrem exact). Sau dacă știm că în original apar coduri (ex: "Art. 123") și traducerea le-a modificat (de exemplu a schimbat ordinea cuvintelor), le putem reformat după modelul original. Aceste ajustări pot fi custom per domeniu.
•	Enforce terminologie preferată: Dacă avem un glosar de termeni specifici domeniului, putem parcurge textul tradus și înlocui termenii traducerii cu varianta preferată. Exemplu: în domeniul medical, dacă știm că "heart" ar trebui tradus mereu "inimă" și nu "cord", putem căuta cuvântul "cord" și dacă contextul e medical, îl înlocuim cu "inimă". Sau în legal, "shall" tradus greșit cu "ar trebui" îl înlocuim cu "va". Aceste reguli fine pot fi ținute într-un fișier de mapping (JSON de tip "cuvant_incorect" -> "cuvant_corect" per domeniu).
•	Verificări de calitate: Implementăm și un scoring de calitate de bază: de pildă, comparăm lungimea textului original vs tradus sau folosim o metrică de similaritate (fuzzy). Putem verifica dacă toate numerele din original apar în traducere (ex: dacă "2023" lipsește din traducere e o problemă). De asemenea, dacă sunt nume proprii sau acronime, verificăm că s-au păstrat (sau transcris corect). Putem face o listă de cuvinte care nu ar trebui traduse (nume, acronime) și să verificăm că ele apar în output; altfel scădem scorul. Un quality_score simplu (0-100) poate fi calculat pe baza acestor heuristici. Acest scor îl putem loga sau afișa (intern) ca să știm cât de sigură e traducerea. Dacă scorul e foarte mic, putem chiar decide să încercăm traducerea cu al doilea motor (fallback) și să alegem varianta cu scor mai bun – deși asta e opțional și poate crește timpul.
•	Context enforcement final: Rezultatul final, după post-procesare, ar trebui să reflecte cât mai fidel contextul dorit. Traducerea specializată astfel obținută va fi livrată clientului. Vom marca în sistem că acel job a fost tradus cu contextul X, poate salvăm atât varianta brută cât și varianta ajustată – util dacă vrem ulterior să vedem ce a modificat post-procesarea.
Prin aceste tehnici (prompting și post-editare automată), motorul de traducere va produce texte mult mai adecvate contextului specializat. Avantajele așteptate sunt menționate și în analiza de design: +30% acuratețe și sensuri corecte pentru termeni tehnici, stil adaptat formal/informal unde e cazul[25][3]. În plus, realizăm asta fără a plăti pentru un API special de context – folosim la maxim capacitățile gratuit disponibile ale motoarelor existente, completate de inteligența noastră locală.
3.3 Pricing Diferențiat în Funcție de Context (Smart Pricing 1.0)
•	Coeficienți pe domeniu: Vom implementa un sistem de calcul al costului dinamic al traducerii, care ține cont de domeniul (contextul) documentului. Ideea este că traducerile specializate (medical, juridic, tehnic) vor fi taxate puțin mai mult decât cele generale, datorită complexității și valorii adăugate[30]. Conform strategiei business, prețul de bază stabilit este ~7 RON/pagină pentru general și poate urca la ~8-9 RON/pagină pentru contexte complexe[31]. Vom modela acest lucru prin coeficienți. De exemplu:
•	Domeniu general: coeficient 1.0 (preț de bază).
•	Juridic/Medical: coeficient 1.2 (adică +20% față de bază).
•	Tehnic/IT: coeficient 1.1 (+10%).
•	Business/Marketing: coeficient 1.1.
•	Academic/Educațional: coeficient 1.05, etc.
Valori exacte vor fi puse în fișierul de configurare (ex: pricing.yaml). Aceste multipliers reflectă și sugestiile GPT-5 (ex: medical ~1.3, OCR ~1.35, expres ~1.7 dacă am include și factori de urgență)[32][21], deși pentru MVP vom folosi doar componenta de domeniu acum, restul (layout, SLA) fiind eventual pe viitor.
•	Formula de calcul: Formula generală sugerată este:
 	 \text{preț} = \text{nr_cuvinte} \times \text{tarif_bază_per_cuvânt} \times \text{coef_domeniu} \times \text{coef_layout} \times \text{coef_SLA}
 	[21]. În implementarea actuală (Batch 1), vom considera layout = 1 (nu diferențiem cost dacă e PDF scanat sau nu, deocamdată) și SLA = 1 (nu avem încă opțiuni de livrare rapidă). Astfel se reduce la:
 	 \text{preț} = \text{nr_cuvinte} \times \text{tarif_bază} \times \text{coeficient\_domeniu}.
 	Vom stabili tarif_bază astfel încât pentru general să iasă ~7 RON/pag. Dacă considerăm 300 cuvinte = 1 pagină, 7 RON/pag implică ~0.0233 RON/cuvânt. Putem rotunji la 0.025 RON/cuvânt tarif de bază (25 bani/100 de cuvinte). Atunci pentru domeniu medical (coef 1.2) rezultă ~0.03 RON/cuvânt, adică ~9 RON/pag – în linie cu planul. Vom preciza tariful exact în config pentru transparență.
•	Calculator de preț instant: Pe front-end (pagina web), vom adăuga un calculator dinamic care afișează clientului costul estimat imediat ce știm numărul de cuvinte și domeniul:
•	După upload, backend-ul va conta cuvintele (componentă deja existentă în procesor) și va transmite valoarea către front (poate deja se întâmplă). Vom extinde să transmită și prețul calculat sau factorii.
•	Alternativ, putem face calculul direct în JavaScript: la upload, front-end-ul primește numărul de cuvinte și apoi pe baza domeniului selectat aplică formula. Pentru asta, putem expune coeficienții și prețul per cuvânt într-un obiect JS (sau ca dată într-un atribut).
•	Simplu: includem în pagina generată un snippet de genul:
 	const base_rate = 0.025;
const domain_multipliers = { general: 1.0, juridic: 1.2, medical: 1.2, tehnic: 1.1, it: 1.1, economic: 1.1, academic: 1.0, sport: 1.0 };
// Apoi cand avem word_count:
let price = (word_count * base_rate * domain_multipliers[selectedDomain]).toFixed(2);
document.getElementById('price_display').innerText = price + " RON";
•	Astfel, când utilizatorul schimbă dropdown-ul de domeniu, prețul se recalculează instant. Acest preview pricing îi oferă transparență și poate influența alegerea (ex: dacă vede că "general" e mai ieftin cu 10%, ar putea totuși opta pentru "general", dar atunci calitatea scade – decizie a lui).
•	A/B testing pricing: Vom lăsa loc pentru experimentare – de ex. definim două scheme de prețuri (Strategy A și B) pentru a vedea care performează mai bine comercial. În config putem avea doi seturi de coeficienți sau tarife. Implementarea:
•	Fiecare nou utilizator (sesiune nouă) este aleator asignat unei strategii (A sau B). Putem face if random() < 0.5 then strategy=A else B și stocăm în session['pricing_strategy'].
•	Strategia A ar putea fi cea descrisă (ex: base 0.025, coef medical 1.2), Strategia B poate avea base mai mic și coeficienți mai mari, sau viceversa, ori chiar fără diferențiere pe context (ca test extrem).
•	Front-end-ul va folosi valorile conform strategiei curente.
•	Noi vom colecta date despre conversii sub fiecare strategie (vezi mai jos) și putem astfel determina dacă pricing-ul diferențiat aduce mai mult profit sau nu.
•	La nevoie, putem activa/dezactiva A/B din config (ex: un flag enable_ab_test: true și definirea ambelor profile de preț).
•	Analytics în SQLite: Vom crea o tabelă quotes sau orders (dacă nu există deja una) unde vom salva informații despre fiecare ofertă de preț și rezultat:
•	Câmpuri: id, timestamp, user_email (dacă disponibil), words, domain, price_offered, strategy, conversion.
•	conversion poate fi boolean sau o dată – dacă oferta s-a concretizat într-o comandă plătită. Inițial, la generarea ofertei (când user vede prețul și face preview), conversion=0. Dacă userul plătește ulterior, atunci la confirmarea plății actualizăm acea intrare marcând conversion=1.
•	Legarea se poate face printr-un ID de sesiune sau token comanda. De exemplu, generăm un order_id la upload, îl includem în sesiune și-l propagăm și către formularul de plată; la webhook Stripe când vine confirmarea, găsim recordul și marcăm convertit.
•	Pe baza acestor date, ulterior putem calcula rata de conversie pentru fiecare categorie de preț: dacă observăm că strategia A convertește 10% și B doar 5%, alegem A permanent; sau dacă domeniul "medical" cu preț mai mare are conversii mult mai scăzute, poate trebuie redus coeficientul.
•	Aceste analytics vor fi eventual extrase în rapoarte (script separat) – ex: număr de comenzi per domeniu, venit total per domeniu, etc.
În ansamblu, Smart Pricing 1.0 asigură că monetizăm corect valoarea adăugată a traducerilor specializate[21]. Clienții văd clar diferența (preview-ul context vs general) și pot decide informat, iar noi ne asigurăm venituri mai mari unde e cazul, fără a exclude opțiunea mai ieftină. Această diferențiere competitivă a fost subliniată ca un factor cheie de creștere a veniturilor (+25-40% conform estimărilor)[33].
3.4 Previzualizare (Preview) cu Watermark “NEPLĂTIT”
•	Generare preview 200 cuvinte: La upload-ul unui document, sistemul va produce automat o previzualizare gratuită a traducerii – primele ~200 de cuvinte traduse[34]. Acest preview permite clientului să vadă calitatea traducerii înainte de plată. Implementarea concretă: după ce textul e extras și contorizat, luăm primele N caractere/ cuvinte din text (până la sfârșitul propoziției apropiat) și rulăm prin motorul de traducere exact ca la o traducere normală (incluzând adaptarea la context dacă contextul e selectat). Rezultă un fragment tradus.
•	Watermark vizual “PREVIEW - NEPLĂTIT”: Pentru ca acest fragment să nu poată fi utilizat ca atare în scopuri oficiale și să indice clar că este doar un preview, vom suprapune un watermark text vizibil. Modalitatea depinde de cum prezentăm preview-ul:
•	În pagina web (HTML): Putem afișa textul tradus într-un container <div> peste care, folosind CSS position: absolute; opacity: 0.3; transform: rotate(-30deg);, desenăm repetitiv textul "PREVIEW - NEPLĂTIT". Practic, facem un element overlay pe tot <div>-ul cu conținut, cu pointer-events none, care conține watermark-ul (de ex. într-un <canvas> sau ca text repetat). Astfel utilizatorul poate citi textul, dar dacă încearcă să-l copieze, fie copiază și watermark-ul (dacă e parte a textului), fie în cazul overlay-lui pur poate totuși copia textul dedesubt. Pentru a împiedica extragerea complet curată, putem insera cuvintele "PREVIEW" direct în textul tradus la fiecare câteva propoziții. O idee: inserăm sintagma "[PREVIEW]" între paranteze pătrate la fiecare 2-3 propoziții în text. Aceasta se poate elimina manual, dar e incomod pentru un fragment mai lung.
•	În fișier descărcabil (PDF/DOCX): Dacă oferim posibilitatea de descărcare a preview-ului, ar fi ideal să fie un PDF cu watermark. Putem genera un PDF simplu din text folosind o bibliotecă ca ReportLab (free) sau chiar pandoc/LaTeX dacă avem, dar ReportLab e ok. Vom pune textul tradus pe pagină și vom desena deasupra (pe layer-ul PDF) textul "PREVIEW - NEPLĂTIT" diagonal și transparent de mai multe ori. În Word (DOCX) e puțin mai complicat de generat watermark, dar Python-docx permite inserarea unei forme sau text în header ca watermark.
•	Cel mai simplu: generăm PDF cu watermark. Apoi pe front-end, butonul "Descarcă preview" oferă acel PDF. PDF-ul fiind ne-editabil ușor, watermark-ul rămâne protejat.
•	Comparație standard vs. context: O funcționalitate de previzualizare comparativă poate evidenția clientului beneficiile contextului specializat:
•	Dacă userul a ales (sau auto-detectat) un domeniu specializat, putem genera două versiuni ale preview-ului:
a.	Traducere standard (general) – adică fără prompt special, ca și cum contextul ar fi "General".
b.	Traducere contextualizată – cu adaptările de la 3.2.
•	Apoi în interfață putem afișa fie una sub alta, fie într-un tab switcher: "Previzualizare Standard" vs "Previzualizare cu Context Medical". Eventual evidențiem diferențele: de exemplu, colorăm cu verde cuvintele care diferă între versiuni sau folosim un diff highlight simplu. Dacă diferențele sunt semnificative (de obicei la termeni tehnici), clientul va vedea concret că versiunea contextualizată e mai bună (sperăm).
•	Dacă domeniul este General oricum, putem omite comparația (sau afișa doar o singură variantă cu notă că pt general nu există diferențe).
•	Această funcție are și rol de marketing: arată ce face contextul specializat. Ar fi util mai ales dacă vom oferi Industry Packs premium – ex: poate în varianta gratuită ar primi traducerea generală, și li se arată cum ar arăta dacă ar plăti pack-ul medical. Dar în cazul de față, cum oricum oferim context in preț, e mai mult pentru convingere și transparență.
•	Download sample: După afișarea preview-ului în browser, oferim opțiunea de download ("Descarcă acest preview ca PDF"). Cum am descris, generăm PDF-ul cu watermark. Alternativ, chiar direct butonul poate trimite header de PDF la browser. Astfel, userul poate arăta și altcuiva (ex: decidentului) calitatea traducerii, având totodată watermark care indică că nu e finală.
•	Tracking conversie: Vom folosi datele despre preview în analiza de conversie din 3.3. Adică atunci când generăm preview, înregistrăm asta (event preview_generated în log audit + în tabela quotes punem price, domain etc.). Dacă userul nu finalizează comanda, vom ști. Putem chiar adăuga în DB un flag preview_shown=1 pentru comenzi. În rapoarte, putem verifica câți useri au renunțat după preview – de exemplu, dacă renunță mulți, poate calitatea a fost nesatisfăcătoare sau prețul prea mare.
•	De asemenea, putem track-ui cât timp a stat pe pagina de preview sau dacă a descărcat PDF-ul (dacă accesarea link-ului de download generează un log). Aceste micro-analitice pot ajuta să identificăm interesul: dacă cineva a descărcat preview dar nu a cumpărat, poate l-a folosit în alt scop? (sau am pus prea mult din traducere? 200 cuvinte e ~ o jumate de pagină, e ok).
•	Oricum, conversia finală (plată efectuată) am discutat că o marcăm. Putem calcula preview-to-sale conversion rate.
Implementarea acestei previzualizări cu watermark consolidează încrederea clientului în serviciu (văd ce primesc) și totodată protejează afacerea de abuz (clientul nu primește tot textul fără plată). Este o componentă deja prevăzută ca esențială în sistem[34] și cu watermarking profesional conform planificării inițiale[35].
Testele unitare vor verifica că watermark-ul chiar se aplică (ex: după generarea preview, stringul "PREVIEW" apare în text/PDF) și că preview-ul e limitat ca lungime.
3.5 “Client Memory” – Memorie de Traduceri în SQLite
•	Database local pentru memorie: Vom construi o memorie de traducere light (TM-Lite basic) stocată în SQLite, care să rețină fragmente traduse anterior pentru fiecare client. Ideea este să învățăm din traducerile trecute astfel încât viitoarele traduceri pentru același client să fie consistente terminologic și adaptate preferințelor sale. Acest concept de Translation Memory a fost evidențiat ca inovativ (ADN #5) în planul GPT-5[36] – noi implementăm o versiune simplificată acum, fără interfață de aprobare dar cu efect de personalizare.
•	Structura TM: Vom crea o tabelă SQLite, de exemplu translation_memory, cu coloe precum:
•	user_id (poate email sau un hash al email-ului clientului, pentru confidențialitate),
•	source_text (textul original al segmentului),
•	translated_text (traducerea finală livrată),
•	domain (domeniul/contextul, dacă relevant terminologic),
•	eventual created_at.
•	Această tabelă va stoca segmente de propoziție sau fraze scurte. Nu vom stoca întreg documentul ca o intrare (căci nu ajută la reutilizare parțială).
•	Segmentare propoziții: Când procesăm o traducere finală pentru livrare, vom sparge textul în propoziții (folosind NLTK sent_tokenize sau simple split pe punct/interogație/exclamație, ținând cont de prescurtări). Apoi pentru fiecare propoziție (sau poate paragraf scurt) vom salva în memorie perechea original-tradus. Ca optimizare, putem filtra segmentele foarte scurte (ex: <3 cuvinte, care pot fi comune și nu ne ajută).
•	Dacă textul e foarte lung, stocarea fiecărei propoziții s-ar putea să ducă la o memorie mare, dar SQLite poate gestiona și zeci de mii de rânduri fără probleme. Oricum, memoriile vor fi per client, deci fragmentate.
•	Învățare din alegeri: Client memory va acționa și ca un profil de preferințe:
•	Dacă observăm că un anumit client folosește mereu contextul "Medical", putem să setăm implicit acel context pentru viitoarele sale comenzi (pre-selectare în dropdown) – asta poate fi considerată o preferință învățată. Putem avea o tabelă user_preferences cu user, preferred_domain, preferred_tone etc., actualizată după fiecare comandă (ex: dacă 3 traduceri la rând au fost Medical, setăm preferința Medical).
•	Dacă ar exista opțiuni de stil (ex: formal vs informal default), le-am putea memora la fel.
•	O altă preferință implică terminologia: dacă la o comandă userul a făcut observații (de ex printr-un sistem de feedback) că preferă un anumit termen, ideal am reține. În lipsa interacțiunii directe, ne bazăm doar pe ce traduceri a acceptat (presupunem că cele livrate au fost ok).
•	Reutilizare automată a traducerilor: Când un client încarcă un document nou, vom încerca să reutilizăm segmente din memoria lui:
•	După ce extragem textul documentului, îl vom parcurge propoziție cu propoziție. Pentru fiecare propoziție, căutăm în translation_memory dacă același source_text există deja pentru acel user_id. Dacă da, înseamnă că am tradus exact acea propoziție în trecut. Putem atunci înlocui direct traducerea cu cea din memorie, în loc să o retraduce motorul AI. Acest lucru asigură consistență 100% cu traducerile anterioare (dacă atunci a fost corectă și acceptată).
•	Dacă găsim potrivire parțială (propoziții foarte asemănătoare dar nu identice), nu vom înlocui automat (risc de nepotrivire). Dar putem marca acea propoziție ca potențial candidat și eventual să evidențiem într-un raport de post-procesare pentru translator uman (dacă ar fi). În contextul complet automat, momentan vom ignora fuzzy match pentru simplitate, focus pe exact match.
•	Căutarea exactă în SQLite este rapidă dacă indexăm source_text. Putem folosi interogare SELECT translated_text FROM translation_memory WHERE user_id=? AND source_text=?. Pentru fuzzy, am putea utiliza full-text search FTS5 și un algoritm de similaritate, dar lăsăm ca posibil upgrade viitor (TM-Lite extins).
•	După această înlocuire, textul (sau segmentele) rămase netraduse vor fi trimise ca de obicei la DeepL/Google. Apoi vom avea un mix: unele propoziții traduse nou, altele luate din memorie. Le combinăm în ordinea originală pentru a forma rezultatul final.
•	Personalizare cu algoritmi de scor: Dacă există multiple traduceri candidate în memorie pentru același text sursă (situație rară - de ex. dacă același string a apărut în două documente diferite și poate traducerile au fost diferite din cauza contextului), va trebui să alegem cea mai potrivită. Putem atribui un scor de încredere fiecărei memorii:
•	Traducerile realizate cu context același ca documentul curent au prioritate (dacă sursa e identică dar una traducere era sub context Medical și alta sub General, iar acum tot medical context, alegem varianta Medical).
•	Traducerile mai recente ar putea fi mai de încredere – deci putem lua ultima traducere (ORDER BY created_at DESC LIMIT 1).
•	În viitor, dacă am avea un sistem de rating/corrections de la client, am marca în memorie cele validate vs cele doar generate. În lipsa asta, presupunem toate livrate sunt ok.
•	Analytics comportament: Vom colecta date și despre modul în care memory e folosită:
•	Câte segmente au fost reutilizate într-o nouă traducere (ex: "20% din propoziții traduse instant din memorie") – aceste economii pot fi logate în audit (event "memory_usage" cu procent).
•	Dacă vom avea feedback de la user (ex: userul a suprascris totuși traducerea oferită - asta nu prea se întâmplă în sistemul automat decât dacă cere revizuire manual), am putea folosi asta să corectăm memoria.
•	Oricum, putem genera periodic un raport (script Python) care analizează tabela translation_memory: câte intrări pe user, ce domenii, rata de reutilizare etc. Aceste informații pot fi salvate în JSON/CSV pentru Business Intelligence (face parte și din Batch 6 Analytics planificat).
În esență, Client Memory conferă sistemului o capacitate de învățare continuă din documentele anterioare ale fiecărui client, realizând personalizare și consistență. Acest lucru duce la traduceri mai coerente și la fidelizarea clienților (ei vor ști că terminologia lor preferată va fi menținută)[36]. Deși implementarea noastră e de bază, ea pune fundația pentru un eventual modul TM-Lite complet (cu interfață de editare, aprobare termeni etc.)[37]. Important, totul este local, stocat în SQLite și folosit automat, deci fără costuri suplimentare dar adăugând un USP tehnologic serviciului.
📦 Deliverables și Plan de Implementare
Toate componentele de mai sus vor fi livrate sub forma unui pachet complet (cod sursă, scripturi, documentație) pregătit pentru self-hosting. Structura livrării și artefactele incluse:
•	Codul sursă complet (Python + Frontend): Vom furniza întregul proiect într-o structură organizată (de ex. similar cu cea din documentația finală[38]). Codul va include:
•	Actualizări în modulul Flask principal (backend/app.py și rutele) pentru a integra webhook-ul Stripe securizat, upload-ul local, portalul DSAR și rate-limiter-ul.
•	Noi module Python: ex. security.py (pentru HMAC verificație și rate limit middleware), dsar_service.py (pentru logica de export/ștergere date), memory.py (pentru funcții de memorare și reutilizare traduceri), pricing.py (cu formula de calcul preț).
•	Modificări la translation_service.py (adaptare context prompt și post-procesare), document_processor.py (pentru a apela detectarea contextului și a segmenta textul pt memory), payment_service.py (integrare idempotency webhook).
•	Frontend: fișierul HTML (frontend/index.html) va fi actualizat cu dropdown de context, afișare preț dinamic și secțiune DSAR (sau link către portal DSAR). De asemenea, vom include un CSS cu stilul watermark și eventual pagini suplimentare (frontend/dsar.html pentru confirmări).
•	Fiecare fișier sursă va conține comentarii detaliate în limba engleză (sau română, după preferință) explicând logica, pentru ușurința mentenanței.
•	Baza de date SQLite + scripturi migrare: Vom furniza un fișier schema.sql sau migrări incremental (dacă se folosește Alembic) care să creeze/actualizeze tabelele necesare:
•	Tabele noi: webhook_events (id procesat), rate_limits/banned_ips (dacă e cazul, deși acestea se pot ține doar în memorie și config), dsar_requests, translation_memory, user_preferences, quotes (sau extindere la tabelă orders existentă cu câmpuri noi).
•	Tabele existente: dacă există orders/uploads etc., vom modifica să adăugăm coloana context selectat, poate word_count, price etc. Vom documenta aceste modificări.
•	Un script de migrare (ex: migrate_1_0_to_1_1.py) poate fi inclus pentru a popula noile tabele sau a migra date (dacă MVP-ul existent avea deja structuri similare).
•	Baza de date va rămâne SQLite pentru simplitate, dar codul va fi scris în așa fel încât comutarea la PostgreSQL (dacă se dorește în producție mai târziu) să fie relativ simplă, dat fiind că vom folosi SQL standard și ORM minimal (posibil folosim direct SQLAlchemy core pentru abstracție, dacă e deja în proiect).
•	Fișiere de configurare YAML/ENV: Toate setările vor fi externalizate în fișiere de config, astfel încât sistemul e ușor de ajustat:
•	config/settings.yaml – conține chei precum: STRIPE_WEBHOOK_SECRET, DEEPL_API_KEY, GOOGLE_API_KEY, BASE_PRICE_PER_WORD, DOMAIN_COEFFICIENTS, RATE_LIMITS (ex: general:5/min, preview:2/day), WHITELIST_IPS, SMTP_SERVER + credențiale, etc.
•	config/contexts.yaml – definirea domeniilor și parametrii de traducere (prompt, formality, terminologie). Alternativ aceasta poate fi integrată în settings.yaml.
•	config/keywords/ – un folder cu JSON-urile de cuvinte-cheie per domeniu (dacă nu punem direct în code).
•	.env.template – un template pentru variabile de mediu sensibile (chei API, parole SMTP). Developerul poate copia .env.template în .env și completa valorile reale.
•	Vom asigura că niciun secret nu e hardcodat în cod – totul vine din config sau env, facilitând schimbarea lor fără modificarea codului (important pentru securitate).
•	Sistem de management fișiere locale: Directorii dedicați (uploads/, processed/, logs/, backups/) vor fi creați și menționați în README. Vom oferi scripturi pentru curățare periodică (ex: scripts/cleanup_uploads.py) pe care userul le poate adăuga în cron dacă nu folosim scheduler intern. De asemenea, logs/ și backups/ vor conține fișiere compresate conform rotației configurate.
•	Dockerfile și docker-compose: Vom livra un Dockerfile configurat pentru a rula aplicația Flask (probabil sub gunicorn pentru producție) într-un container Linux. Va include toate dependențele instalațe (Python, librării pip, eventual NLTK data pre-downloaded, etc.). De asemenea, un docker-compose.yml va fi inclus, definind serviciile:
•	web (containerul Flask),
•	redis (container oficial Redis, pentru rate limiting),
•	nginx (opțional, dacă dorim să servim cu un reverse proxy din start; putem include configurarea lui pentru SSL),
•	eventual un serviciu worker (dacă folosim Celery sau altceva pentru task-uri asincrone, dar în MVP probabil nu e nevoie, totul e sync).
•	Compose-ul va lega volume pentru uploads/ și db.sqlite astfel încât datele să persiste și să poată fi accesate/backupate ușor de pe host.
•	Astfel, un utilizator poate rula docker-compose up -d și are întregul sistem pornit (self-hosted). Important: documentația va specifica cum să introducă în .env cheile necesare înainte de pornire.
•	Testare unitară (unittest): Vom furniza un set de teste de bază (testing/test_suite.py și eventual teste separate pe module):
•	Teste pentru verify_stripe_signature() (cu un payload și secret cunoscut, să vedem că valida corect și că respinge la secret greșit sau timestamp expirat).
•	Teste pentru rate limiting logic (simulează 6 cereri de la același IP și verifică că a 6-a e blocată).
•	Teste pentru upload validator (încarcă un fișier dummy .exe renumit .pdf și verifică că e respins).
•	Test pentru funcția de detectare context (dă un text medical și vede că returnează "Medical" peste threshold).
•	Test pentru pricing (calculează preț pt X cuvinte în domeniu Y și verifică formula).
•	Test pentru memory reuse (populează memoria cu o propoziție și vede că la traducerea unui text ce conține propoziția aceea, outputul conține traducerea memorată).
•	Test pentru DSAR export logic (inserează date de test pentru un email, cere export și vede că zip-ul conține fișierele și CSV-urile așteptate).
•	Aceste teste asigură că componentele cheie merg și previn regresii. Le vom documenta rata de acoperire (vizăm >80% funcții critice acoperite).
•	Documentație de utilizare (README): Un fișier README.md detaliat va fi inclus, explicând:
•	Setup inițial: cum se copiază .env și se pun cheile Stripe, DeepL etc., cum se configurează (ex: adăugarea IP-urilor whitelist, ajustarea coeficienților de preț).
•	Instalare locală: fie modul manual (instalare Python 3.X, pip install -r requirements.txt, NLTK download punkt, pornire python backend/app.py), fie modul Docker (docker-compose up). Vom oferi ambele opțiuni.
•	Utilizare și flux: cum se folosește aplicația – de la upload fișier la plată, unde se găsesc fișierele, ce primește userul.
•	Explicații configurări securitate: de ex: cum să schimbi pragurile de rate-limit, cum să adaugi un user premium în whitelist.
•	Proceduri de mentenanță: backup la db.sqlite (poate scriem un script care face dump zilnic), rotație log, cum se extinde lista de cuvinte pentru context, etc.
•	Self-hosting guidance: noțiuni de rulare în producție – ex: recomandarea de a pune serverul în spatele Nginx (dăm exemplu de config), să folosească HTTPS (inclus un script Let’s Encrypt eventual dacă e relevant), și modul de a seta variabilele de mediu pentru Docker.
•	Checklist de securitate: Vom furniza un document/checklist enumerând toate măsurile de securitate implementate și recomandări pentru administratori la deploy:
•	Folosirea SSL (asigurarea că site-ul rulează pe HTTPS; vom include în docker-compose un exemplu cu Nginx + certificatul).
•	Schimbarea cheilor implicite și a parolelor din config (ex: secret Stripe, API keys, secret key Flask pentru sesiuni).
•	Limitarea accesului la portul Redis din exterior (în compose lăsăm Redis accesibil doar intern, sau daca e instalare manuală recomandăm firewall blocare port 6379 extern).
•	Setarea permisiunilor pe fișierele sensibile (config, baza de date – să nu fie world-readable).
•	Regular patching: menționăm versiunile de dependențe folosite (toate sunt gratuite) și recomandăm update periodic.
•	Protecție XSS/CSRF: notăm că folosim Flask cu template autoescape pentru output, și că formularele sensibile (DSAR) vor avea token CSRF dacă e cazul (Flask-WTF poate fi integrat gratuit).
•	Monitorizare: recomandăm să se monitorizeze log-urile de securitate (și pot folosi fail2ban pentru log-ul de ban IP de exemplu).
•	Backup: sfaturi de backup regulat la SQLite și volume uploads, eventual folosirea scriptului de archive logs.
•	Antivirus: sugerăm opțional rularea unui ClamAV periodic pe folderul de upload (script separat).
Acest checklist va fi inclus probabil în README sau ca SECURITY.md. El asigură că cine instalează soluția nu omite pași cruciali de securitate operatională.
•	Integrare completă cu knowledge base existent: Ne vom asigura că tot ce implementăm este compatibil cu planul și structura proiectului existent (MVP-ul de până acum). Vom verifica punct cu punct cerințele extrase din knowledge base și le-am acoperit pe toate:
•	Webhook HMAC + Rate limit + Upload security – acoperite la Componenta 1[1][2].
•	GDPR basic (retenție, export/delete) – acoperite la Componenta 2[16].
•	Context specializat (detectare + adaptare) – acoperit la 3.1 și 3.2[25][3].
•	Smart Pricing + Preview watermark – acoperite la 3.3 și 3.4[21][39].
•	Memory client – acoperit la 3.5 (inspirat de TM-Lite)[36].
Astfel Batch 1 (Foundation & Security + ADN #1) va fi livrat complet, îndeplinind 100% din cerințele stabilite.
•	Arhivă ZIP livrabilă: Toate fișierele vor fi comprimate într-un fișier ZIP structurat (sau direct push pe repository GitHub). Arhiva va conține directoarele menționate (backend/, frontend/, config/, deployment/, logs/, uploads/ etc. – ultimele două goale sau cu un README). Practic reflectă structura din documentația finală[38]. Se va putea extrage și rula imediat conform instrucțiunilor.
Cu aceste deliverables, clientul sau echipa de dezvoltare poate testa și lansa imediat sistemul pe infrastructura proprie, având asigurate atât funcționalitățile cheie cât și ghidajul necesar. Implementarea este robustă, gratuită și la nivel enterprise, îndeplinind obiectivul de a obține un sistem de traduceri AI self-hosted de înaltă calitate, fără costuri de licențiere sau servicii externe – practic transpunând recomandările GPT-5 în soluții concrete 100% open-source[40][41].
________________________________________
[1] [2] [3] [4] [8] [9] [10] [11] [15] [16] [17] [18] [20] [21] [22] [23] [24] [25] [26] [27] [30] [31] [32] [33] [35] [36] [37] [39] [40] [41] 3.1_ideei_functii_web.md
https://drive.google.com/file/d/1zZS3HMHos75UuP_TCJXW_VC4i_ykCCMT
[5] [6] [7] Receive Stripe events in your webhook endpoint | Stripe Documentation
https://docs.stripe.com/webhooks
[12] [13] [14] [19] [28] [29] [34] [38] final_documentation.md
https://drive.google.com/file/d/1omOAawoHvg4Rj2xqtdMeSINPnIPtkwoo